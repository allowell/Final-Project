---
title: "Modeling"
format: html
editor: visual
---

## Introduction

For this project, I will be exploring a dataset that contains binary information related to diabetes. This data comes from a survey with 253,680 responses, and I am primarily interested in investigating whether the presence of diabetes can be predicted based off of other health/personal factors. The response variable is Diabetes_binary, which is a 0 if no diabetes, 1 if prediabetes, or 2 if diabetes. Other variables of interest include HighBP (binary marker for high blood pressure), HighChol (binary marker for high cholesterol), BMI, Smoker (binary marker for smoking at least 100 cigarettes in your life or not), Sex (0 = female, 1 = male), and Age (according to a factored level). In this Modeling section, I intend to find the most optimal model to predict the Disease_binary variable.

## Splitting the data

Loading in all required packages for this document first

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(tidymodels)
library(yardstick)
library(rsample)
library(glmnet)
library(ranger)
```

Next, loading in the data

```{r}
diab_data <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")
diab_data <- diab_data |>
  #Converting a few variables of interest to factor
  mutate(Diabetes_binary = as_factor(Diabetes_binary),
         HighBP = as_factor(HighBP),
         HighChol = as_factor(HighChol),
         Smoker = as_factor(Smoker),
         Sex = as_factor(Sex),
         Age = as_factor(Age)
         ) |>
  #Removing any rows with NA
  drop_na()
```

To start, I need to split the data into training and testing sets. For this particular investigation, I plan on splitting the original data randomly using the rsample package and using 70% for training and 30% for testing. I am also using a seed so that my results and interpretations are reproducible.

```{r}
set.seed(101)

diab_split <- initial_split(diab_data, prop = 0.7)
train <- training(diab_split)
test <- testing(diab_split)
```

Next, setting the folds that will be used for the model selection of all our following models

```{r}
set.seed(101)

diab_folds <- vfold_cv(train, 5)
```

## Modeling

### Logistic Regression

Logistic regression explanation.

For all models

```{r}
LR_spec <- logistic_reg() |>
  set_engine("glm")
```

#### Model 1: Age and BMI as predictors

```{r}
LR1_rec <- recipe(Diabetes_binary ~ Age + BMI, 
                  data = train) |>
  step_normalize(BMI) |>
  step_dummy(Age)

LR1_wkf <- workflow() |>
  add_recipe(LR1_rec) |>
  add_model(LR_spec)

LR1_fit <- LR1_wkf |>
  fit_resamples(diab_folds, metrics = metric_set(mn_log_loss))
```

#### Model 2: Age, BMI, and HighBP as predictors

```{r}
LR2_rec <- recipe(Diabetes_binary ~ Age + BMI + HighBP, 
                  data = train) |>
  step_normalize(BMI) |>
  step_dummy(Age, HighBP)

LR2_wkf <- workflow() |>
  add_recipe(LR2_rec) |>
  add_model(LR_spec)

LR2_fit <- LR2_wkf |>
  fit_resamples(diab_folds, metrics = metric_set(mn_log_loss))
```

#### Model 3: Age, BMI, HighBP, Age and BMI interaction and HighBP and Age interaction.

```{r}
LR3_rec <- recipe(Diabetes_binary ~ Age + BMI + HighBP, 
                  data = train) |>
  step_normalize(BMI) |>
  step_dummy(Age, HighBP) |>
  step_interact( ~ starts_with("Age_"):BMI + starts_with("Age_"):starts_with("HighBP_"))

LR3_wkf <- workflow() |>
  add_recipe(LR3_rec) |>
  add_model(LR_spec)

LR3_fit <- LR3_wkf |>
  fit_resamples(diab_folds, metrics = metric_set(mn_log_loss))
```

#### Finding the best model

```{r}
rbind(LR1_fit |> collect_metrics(),
      LR2_fit |> collect_metrics(),
      LR3_fit |> collect_metrics()) |>
  mutate(Model = c("Model1", "Model2", "Model3")) |>
  select(Model, everything())
```

As we can see from the log loss metrics, our best model is Model 3 since we choose the model with the lowest log loss metric, though Model 2 is close behind. 

### Classification Tree

```{r}
tree_rec <- recipe(Diabetes_binary ~ Age + BMI + HighBP, 
                  data = train) |>
  step_normalize(BMI) |>
  step_dummy(HighBP)

tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 20,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("classification")

tree_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(tree_mod)

tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(10, 5))

tree_fits <- tree_wkf |> 
  tune_grid(resamples = diab_folds,
            grid = tree_grid,
            metrics = metric_set(mn_log_loss))

tree_best_params <- select_best(tree_fits, metric = "mn_log_loss")

tree_final_wkf <- tree_wkf |>
  finalize_workflow(tree_best_params)
```

### Random Forest

```{r}
rf_spec <- rand_forest(mtry = tune(), trees = 100) |>
  set_engine("ranger") |>
  set_mode("classification")

rf_wkf <- workflow() |>
  add_recipe(tree_rec) |>
  add_model(rf_spec)

rf_fit <- rf_wkf |>
  tune_grid(resamples = diab_folds,
           grid = 3,
           metrics = metric_set(mn_log_loss))

rf_best_params <- select_best(rf_fit, metric = "mn_log_loss")

rf_final_wkf <- rf_wkf |>
  finalize_workflow(rf_best_params)
rf_final_fit <- rf_final_wkf |>
  last_fit(diab_split, metrics = metric_set(mn_log_loss))
```

### Find Best Model

```{r}
rf_final_fit <- rf_final_wkf |>
  last_fit(diab_split, metrics = metric_set(mn_log_loss))

class_final_fit <- tree_final_wkf |>
  last_fit(diab_split, metrics = metric_set(mn_log_loss))

log_final_fit <- LR3_wkf |>
  last_fit(diab_split, metrics = metric_set(mn_log_loss))

rbind(log_final_fit |> collect_metrics(), 
      class_final_fit |> collect_metrics(), 
      rf_final_fit |> collect_metrics()) |>
  mutate(Model = c("Logistic Regression", "Classification Tree", "Random Forest")) |>
  select(Model, everything())
```
As we can see, the model with the lowest log loss is the random forest model, with a log loss of 0.3411.

```{r}
#Fit the random foret on the entire dataset
rf_final_wkf_fit<- rf_final_wkf |>
  fit(data = diab_data)
#Saving our worfklow to avoid rerunning in the API each time
saveRDS(rf_final_wkf_fit, "RF_wkf.rds")
```












[Click here for the Modeling Page](Modeling.html)
