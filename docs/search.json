[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "For this project, I will be exploring a dataset that contains binary information related to diabetes. This data comes from a survey with 253,680 responses, and I am primarily interested in investigating whether the presence of diabetes can be predicted based off of other health/personal factors. The response variable is Diabetes_binary, which is a 0 if no diabetes, 1 if prediabetes, or 2 if diabetes. Other variables of interest include HighBP (binary marker for high blood pressure), HighChol (binary marker for high cholesterol), BMI, Smoker (binary marker for smoking at least 100 cigarettes in your life or not), Sex (0 = female, 1 = male), and Age (according to a factored level). In this EDA, I intend to explore initial relationships between a select number of these variables and the Diabetes factored indicator by observing plots and other exploratory analyses. After this initial data exploration, I will move on to modeling with the goal of creating a model that predicts whether a respondent will have diabetes according to certain variables with high accuracy."
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "For this project, I will be exploring a dataset that contains binary information related to diabetes. This data comes from a survey with 253,680 responses, and I am primarily interested in investigating whether the presence of diabetes can be predicted based off of other health/personal factors. The response variable is Diabetes_binary, which is a 0 if no diabetes, 1 if prediabetes, or 2 if diabetes. Other variables of interest include HighBP (binary marker for high blood pressure), HighChol (binary marker for high cholesterol), BMI, Smoker (binary marker for smoking at least 100 cigarettes in your life or not), Sex (0 = female, 1 = male), and Age (according to a factored level). In this EDA, I intend to explore initial relationships between a select number of these variables and the Diabetes factored indicator by observing plots and other exploratory analyses. After this initial data exploration, I will move on to modeling with the goal of creating a model that predicts whether a respondent will have diabetes according to certain variables with high accuracy."
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "Exploratory Data Analysis",
    "section": "Data",
    "text": "Data\n\nImporting the Data\n\nlibrary(tidyverse)\ndiab_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\ndiab_data &lt;- diab_data |&gt;\n  #Converting a few variables of interest to factor\n  mutate(Diabetes_binary = as_factor(Diabetes_binary),\n         HighBP = as_factor(HighBP),\n         HighChol = as_factor(HighChol),\n         Smoker = as_factor(Smoker),\n         Sex = as_factor(Sex),\n         Age = as_factor(Age)\n         ) |&gt;\n  #Removing any rows with NA\n  drop_na()"
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "Exploratory Data Analysis",
    "section": "Summarizations",
    "text": "Summarizations\nThe variables I am most interested in exploring in this EDA are BMI, Age and HighBP. After doing some research on diabetes, I found that these factors are included in the CDC’s Diabetes risk factors (https://www.cdc.gov/diabetes/risk-factors/index.html). Specifically, being overweight and being over the age of 45 are listed as factors that can influence if a person has diabetes, so I felt that BMI and Age were good factors to include to improve prediction. I also found that diabetes and high blood pressure often occur together, which is why I felt it was important to include that as another variable (https://www.hopkinsmedicine.org/health/conditions-and-diseases/diabetes/diabetes-and-high-blood-pressure).\n\nUnivariate Explorations\nFirst, exploring the counts and percentages of the binary diabetes varible (our response).\n\ntable(diab_data$Diabetes_binary)\n\n\n     0      1 \n218334  35346 \n\nnone &lt;- sum(diab_data$Diabetes_binary == 0)/nrow(diab_data)\ndiabetes &lt;- sum(diab_data$Diabetes_binary == 1)/nrow(diab_data)\nc(none, diabetes)\n\n[1] 0.860667 0.139333\n\n\nWe can see that about 86% of the total respondents of this survey did not have diabetes, whereas aroud 14% did have diabetes.\nNext, looking at our binary variable for High Blood Pressure\n\ntable(diab_data$HighBP)\n\n\n     0      1 \n144851 108829 \n\nno_high_bp &lt;- sum(diab_data$HighBP == 0)/nrow(diab_data)\nhigh_bp &lt;- sum(diab_data$HighBP == 1)/nrow(diab_data)\nc(no_high_bp, high_bp)\n\n[1] 0.5709989 0.4290011\n\n\nWe can see that around 57% of the people who responded to this survery did not have high blood pressure, whereas around 43% did have high blood pressure. There is a much smaller difference in the number of respondents in each category of this variable than there were for the diabetes variable.\nNext, looking at our factored Age variable\n\ntable(diab_data$Age)\n\n\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n 5700  7598 11123 13823 16157 19819 26314 30832 33244 32194 23533 15980 17363 \n\n\nThere are 13 total age groups, with the most common age groups being 8, 9 and 10.\nLastly for our univariate analyses, we are looking at BMI\n\nsummary_bmi &lt;- diab_data %&gt;%\n  summarise(\n    mean_BMI = mean(BMI, na.rm = TRUE),\n    sd_BMI = sd(BMI, na.rm = TRUE),\n  )\nprint(summary_bmi)\n\n# A tibble: 1 × 2\n  mean_BMI sd_BMI\n     &lt;dbl&gt;  &lt;dbl&gt;\n1     28.4   6.61\n\n\nWe can see that the mean BMI is 28.38, with a standard deviation of 6.61. According to the CDC, this BMI corresponds to overweight for adults. However, since there is such high variability, we might have some outliers influencing this number.\n\n\nBivariate Exploration\nNext, I want to explore how many people in each age category had diabetes.\n\ntable(diab_data$Diabetes_binary, diab_data$Age)\n\n   \n        1     2     3     4     5     6     7     8     9    10    11    12\n  0  5622  7458 10809 13197 15106 18077 23226 26569 27511 25636 18392 12577\n  1    78   140   314   626  1051  1742  3088  4263  5733  6558  5141  3403\n   \n       13\n  0 14154\n  1  3209\n\n\nFor those that do have diabetes, it definitely seems like there is some correlation between older age and having diabetes.\nNext, looking at the binary variable for high blood pressure versus our diabetes variable.\n\ntable(diab_data$Diabetes_binary, diab_data$HighBP)\n\n   \n         0      1\n  0 136109  82225\n  1   8742  26604\n\n\nWe can see that the vast majority of respondents did not have high blood pressure or diabetes, however a large number of respondents with diabetes also had high blood pressure, indicating a relationship between the two as mentioned earlier.\nNext, I would like to explore some visualizations of our variables. Starting with Age colored by diabetes.\n\nggplot(diab_data, aes(x = Age, fill = Diabetes_binary)) +\n  geom_bar() +\n  labs(title = \"Distribution of Diabetes Status\", x = \"Diabetes Status\", y = \"Count\")\n\n\n\n\n\n\n\n\nFor our next graph, I want to look at the boxplots for BMI between the different values of our diabetes binary variable.\n\nggplot(diab_data, aes(x = Diabetes_binary, y = BMI)) +\n  geom_boxplot(fill = \"red\") +\n  scale_x_discrete(labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  labs(title = \"BMI by Diabetes Status\", x = \"Diabetes Status\", y = \"BMI\")\n\n\n\n\n\n\n\n\nAs we can see, the BMI for Diabetes looks to be higher on average, though they do overlap. Both distributions contain a large amount of high outliers.\nNow that we have finished an exploratory data analysis, we will move on to the Modeling portion and attempt to predict the binary diabetes variable using the three variable outlined here.\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "For this project, I will be exploring a dataset that contains binary information related to diabetes. This data comes from a survey with 253,680 responses, and I am primarily interested in investigating whether the presence of diabetes can be predicted based off of other health/personal factors. The response variable is Diabetes_binary, which is a 0 if no diabetes, 1 if prediabetes, or 2 if diabetes. Other variables of interest include HighBP (binary marker for high blood pressure), HighChol (binary marker for high cholesterol), BMI, Smoker (binary marker for smoking at least 100 cigarettes in your life or not), Sex (0 = female, 1 = male), and Age (according to a factored level). In this Modeling section, I intend to find the most optimal model to predict the Disease_binary variable."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "",
    "text": "For this project, I will be exploring a dataset that contains binary information related to diabetes. This data comes from a survey with 253,680 responses, and I am primarily interested in investigating whether the presence of diabetes can be predicted based off of other health/personal factors. The response variable is Diabetes_binary, which is a 0 if no diabetes, 1 if prediabetes, or 2 if diabetes. Other variables of interest include HighBP (binary marker for high blood pressure), HighChol (binary marker for high cholesterol), BMI, Smoker (binary marker for smoking at least 100 cigarettes in your life or not), Sex (0 = female, 1 = male), and Age (according to a factored level). In this Modeling section, I intend to find the most optimal model to predict the Disease_binary variable."
  },
  {
    "objectID": "Modeling.html#splitting-the-data",
    "href": "Modeling.html#splitting-the-data",
    "title": "Modeling",
    "section": "Splitting the data",
    "text": "Splitting the data\nLoading in all required packages for this document first\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(tidymodels)\nlibrary(yardstick)\nlibrary(rsample)\nlibrary(glmnet)\nlibrary(ranger)\n\nNext, loading in the data\n\ndiab_data &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndiab_data &lt;- diab_data |&gt;\n  #Converting a few variables of interest to factor\n  mutate(Diabetes_binary = as_factor(Diabetes_binary),\n         HighBP = as_factor(HighBP),\n         HighChol = as_factor(HighChol),\n         Smoker = as_factor(Smoker),\n         Sex = as_factor(Sex),\n         Age = as_factor(Age)\n         ) |&gt;\n  #Removing any rows with NA\n  drop_na()\n\nTo start, I need to split the data into training and testing sets. For this particular investigation, I plan on splitting the original data randomly using the rsample package and using 70% for training and 30% for testing. I am also using a seed so that my results and interpretations are reproducible.\n\n#Setting a seed for reproducability\nset.seed(101)\n\n#Creating training and testing data\ndiab_split &lt;- initial_split(diab_data, prop = 0.7)\ntrain &lt;- training(diab_split)\ntest &lt;- testing(diab_split)\n\nNext, setting the folds that will be used for the model selection of all our following models\n\nset.seed(101)\n#Creating 5 folds of training data\ndiab_folds &lt;- vfold_cv(train, 5)"
  },
  {
    "objectID": "Modeling.html#modeling",
    "href": "Modeling.html#modeling",
    "title": "Modeling",
    "section": "Modeling",
    "text": "Modeling\n\nLogistic Regression\nNext, we will begin fitting models to find the best predictor of our test set. We will start with logistic regression, which is a type of regression model that lets us fit models based on response variables that are not numeric. We use this when our response is binary or factored, for example. In this dataset, we are interested in predicting the diabetes binary variable, meaning that logistic regression will work well here.\nFirst, setting up the model type and engine that will be used for all three of our potential logistic regression models.\n\nLR_spec &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\")\n\n\nModel 1: Age and BMI as predictors\n\n#Creating recipe for first model\nLR1_rec &lt;- recipe(Diabetes_binary ~ Age + BMI, \n                  data = train) |&gt;\n  step_normalize(BMI) |&gt;\n  step_dummy(Age)\n#Putting it in a workflow\nLR1_wkf &lt;- workflow() |&gt;\n  add_recipe(LR1_rec) |&gt;\n  add_model(LR_spec)\n#Fitting to our CV folds\nLR1_fit &lt;- LR1_wkf |&gt;\n  fit_resamples(diab_folds, metrics = metric_set(mn_log_loss))\n\n\n\nModel 2: Age, BMI, and HighBP as predictors\n\n#Creating recipe for second model\nLR2_rec &lt;- recipe(Diabetes_binary ~ Age + BMI + HighBP, \n                  data = train) |&gt;\n  step_normalize(BMI) |&gt;\n  step_dummy(Age, HighBP)\n#Putting it in a workflow\nLR2_wkf &lt;- workflow() |&gt;\n  add_recipe(LR2_rec) |&gt;\n  add_model(LR_spec)\n#Fitting to our CV folds\nLR2_fit &lt;- LR2_wkf |&gt;\n  fit_resamples(diab_folds, metrics = metric_set(mn_log_loss))\n\n\n\nModel 3: Age, BMI, HighBP, Age and BMI interaction and HighBP and Age interaction.\n\n#Creating recipe for third model using interaction\nLR3_rec &lt;- recipe(Diabetes_binary ~ Age + BMI + HighBP, \n                  data = train) |&gt;\n  step_normalize(BMI) |&gt;\n  step_dummy(Age, HighBP) |&gt;\n  step_interact( ~ starts_with(\"Age_\"):BMI + starts_with(\"Age_\"):starts_with(\"HighBP_\"))\n#Putting it in a workflow\nLR3_wkf &lt;- workflow() |&gt;\n  add_recipe(LR3_rec) |&gt;\n  add_model(LR_spec)\n#Fitting to our CV folds\nLR3_fit &lt;- LR3_wkf |&gt;\n  fit_resamples(diab_folds, metrics = metric_set(mn_log_loss))\n\n\n\nFinding the best model\n\n#Putting our metrics for each model together to find best one\nrbind(LR1_fit |&gt; collect_metrics(),\n      LR2_fit |&gt; collect_metrics(),\n      LR3_fit |&gt; collect_metrics()) |&gt;\n  mutate(Model = c(\"Model1\", \"Model2\", \"Model3\")) |&gt;\n  select(Model, everything())\n\n# A tibble: 3 × 7\n  Model  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 Model1 mn_log_loss binary     0.361     5 0.00182 Preprocessor1_Model1\n2 Model2 mn_log_loss binary     0.347     5 0.00168 Preprocessor1_Model1\n3 Model3 mn_log_loss binary     0.346     5 0.00163 Preprocessor1_Model1\n\n\nAs we can see from the log loss metrics, our best model is Model 3 since we choose the model with the lowest log loss metric.\n\n\n\nClassification Tree\nA classification tree model is used when we want to classify, or predict, whether a value belongs in a specific group or not. It splits the predictor space into regions, and the most prevalent class in the region is used as the prediction. This is helpful in our case as we can use it to predict whether an abservation would have diabetes or not based on the predictors and it will be predicted into that class.\n\n#Creating general tree recipe to use for both classification and Random Forest\ntree_rec &lt;- recipe(Diabetes_binary ~ Age + BMI + HighBP, \n                  data = train) |&gt;\n  step_normalize(BMI) |&gt;\n  step_dummy(HighBP)\n#Making model setup\ntree_mod &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"classification\")\n#Creating workflow\ntree_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_mod)\n#Setting up grid\ntree_grid &lt;- grid_regular(cost_complexity(),\n                          tree_depth(),\n                          levels = c(10, 5))\n#Fitting based on grid according to log loss\ntree_fits &lt;- tree_wkf |&gt; \n  tune_grid(resamples = diab_folds,\n            grid = tree_grid,\n            metrics = metric_set(mn_log_loss))\n#Extracting best parameters from our fits\ntree_best_params &lt;- select_best(tree_fits, metric = \"mn_log_loss\")\n#Finalizing workflow\ntree_final_wkf &lt;- tree_wkf |&gt;\n  finalize_workflow(tree_best_params)\n\n\n\nRandom Forest\nLastly, a random forest model is a regression model that created multiple trees from bootstrap sampling and averages the results to obtain a final prediction. In our case, we will predict the probability that a certain observation falls in each class of the diabetes binary variable. In random forests, not all of the predictors are used at each step, as the number of parameters to use is actually a tuning parameter, and the optimal number will be selected after running the model\n\n#Specifying engine and mode\n#I used trees = 100 because original was taking over an hour to run\nrf_spec &lt;- rand_forest(mtry = tune(), trees = 100) |&gt;\n  set_engine(\"ranger\") |&gt;\n  set_mode(\"classification\")\n#Creating workflow\nrf_wkf &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(rf_spec)\n#Fitting model\nrf_fit &lt;- rf_wkf |&gt;\n  tune_grid(resamples = diab_folds,\n           grid = 3,\n           metrics = metric_set(mn_log_loss))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n#Extracting best parameters\nrf_best_params &lt;- select_best(rf_fit, metric = \"mn_log_loss\")\n#Finalizing workflow\nrf_final_wkf &lt;- rf_wkf |&gt;\n  finalize_workflow(rf_best_params)\n\n\n\nFind Best Model\nTo find the best model, we are going to fit the logistic regression model, classification tree and random forest model onto the testing data and select the best one based on log loss like we did when comparing logistic regression models.\n\n#Fit each model onto test set\nlog_final_fit &lt;- LR3_wkf |&gt;\n  last_fit(diab_split, metrics = metric_set(mn_log_loss))\nclass_final_fit &lt;- tree_final_wkf |&gt;\n  last_fit(diab_split, metrics = metric_set(mn_log_loss))\nrf_final_fit &lt;- rf_final_wkf |&gt;\n  last_fit(diab_split, metrics = metric_set(mn_log_loss))\n\n#Combine metrics together to easily see the best performing model\nrbind(log_final_fit |&gt; collect_metrics(), \n      class_final_fit |&gt; collect_metrics(), \n      rf_final_fit |&gt; collect_metrics()) |&gt;\n  mutate(Model = c(\"Logistic Regression\", \"Classification Tree\", \"Random Forest\")) |&gt;\n  select(Model, everything())\n\n# A tibble: 3 × 5\n  Model               .metric     .estimator .estimate .config             \n  &lt;chr&gt;               &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Logistic Regression mn_log_loss binary         0.344 Preprocessor1_Model1\n2 Classification Tree mn_log_loss binary         0.350 Preprocessor1_Model1\n3 Random Forest       mn_log_loss binary         0.341 Preprocessor1_Model1\n\n\nAs we can see, the model with the lowest log loss is the random forest model, with a log loss of 0.3411. This is therefore the model that we will use in our API!\nNow that we have found our best performing model, we need to fit it to the entire dataset, not just testing, and save that workflow to reuse it in our API to avoid rerunning it.\n\n#Fit the random forest onto the entire dataset\nrf_final_wkf_fit&lt;- rf_final_wkf |&gt;\n  fit(data = diab_data)\n#Saving our workflow to avoid rerunning in the API each time\nsaveRDS(rf_final_wkf_fit, \"RF_wkf.rds\")"
  }
]